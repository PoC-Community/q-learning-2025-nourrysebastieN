{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning\n",
    "### Introduction\n",
    "Today, you'll learn about QLearning using the gym api.\n",
    "> Gym is a standard API for reinforcement learning, and a diverse collection of reference environments.\n",
    "\n",
    "Reinforcement learning is a subset of Artificial Intelligence. Our AI will learn by playing the same game over and over again until it learns what it must do in order to win.\n",
    "\n",
    "In this workshop, you will:\n",
    "\n",
    "1. Learn about Q Tables\n",
    "2. Setup a gym environment\n",
    "3. Train an AI to solve the [FrozenLake environment](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "\n",
    "Let's start by importing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Q-Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example of Q-Learning: \n",
    "\n",
    "![Example of a Q-Learning Environment](./images/example.png)\n",
    "\n",
    "A game where the AI must get to the ‚ÄúEnd‚Äù coordinates on the map in the shortest amount of time while avoiding the bombs and collecting the bonuses.\n",
    "\n",
    "A Q-Table for this environment could be visualized like this. \n",
    "\n",
    "![Example of a Q-Table](./images/qtable.png)\n",
    "\n",
    "With all of the possible actions listed horizontally and all of the possible states listed vertically.\n",
    "\n",
    "Write a function which generates an empty array of shape `x` * `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Initializing an empty table\n",
    "\n",
    "def init_q_table(x: int, y: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function must return a 2D matrix containing only zeros for values.\n",
    "    \"\"\"\n",
    "    return np.zeros((x, y), dtype=float)\n",
    "\n",
    "qTable = init_q_table(5, 4)\n",
    "\n",
    "print(\"Q-Table:\\n\" + str(qTable))\n",
    "\n",
    "assert(np.mean(qTable) == 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning, the AI receives a **reward** each time it **acts**:\n",
    "\n",
    "This reward is a number that will be interpreted by our AI:\n",
    "\n",
    "- If it is negative, the AI will learn that the action it made which led to this reward was probably not the best\n",
    "- If it is null or positive, the AI will learn that the action it made which led to this reward was probably smart and it should do it again.\n",
    "\n",
    "Here, the AI would likely receive a **negative reward** (-1) if it chooses to move to the right (because there's a bomb)\n",
    "\n",
    "However, if it went anywhere else, it would probably receive a **neutral reward** (0), since it would end up on blank squares.\n",
    "\n",
    "Assuming it decided to go to the right, the AI would encounter a bomb and the value for [Start, Move_Right] would decrease.\n",
    "\n",
    "\n",
    "Once we receive a reward, we can use the Q Formula below to update our Q Table's values. Our AI will use these values to learn what the most profitable action is at each state of the environment.\n",
    "\n",
    "![QFormula](./images/qformula.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translation:\n",
    "\n",
    "**Q<sup>new</sup>(s<sub>t</sub>, a<sub>t</sub>)** = the new value for the current State and Action on our Qtable\n",
    "\n",
    "**Q(s<sub>t</sub>, a<sub>t</sub>)** = the old value for the current State and Action on our QTable\n",
    "\n",
    "**Œ± (learning rate)** = a constant value we use for our algorithm's learning speed (usually a number between 0.5 and 0.05)\n",
    "\n",
    "**r<sub>t</sub> (reward)** = the reward received (as per the example above, -1 if you hit a bomb)\n",
    "\n",
    "**ùõæ (discount factor)** = another constant value we use to determine how good of a memory our AI has\n",
    "\n",
    "**max Q<sup>new</sup>(s<sub>t+1</sub>, a)** = the value of the most profitable action at the new state\n",
    "\n",
    "##### Now that you know how a Q Table updates itself using the Q Formula, let's try to implement this formula as a python function:\n",
    "(You can use the `LEARNING_RATE` and `DISCOUNT_RATE` constants in your formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.05\n",
    "DISCOUNT_RATE = 0.99\n",
    "\n",
    "def q_function(q_table: np.ndarray, state: int, action: int, reward: int, newState: int) -> float:\n",
    "    \"\"\"\n",
    "    This function must implement the q_function equation pictured above.\n",
    "\n",
    "    It should return the updated q-table.\n",
    "    \"\"\"\n",
    "\n",
    "    old_value = q_table[state, action]\n",
    "    best_future = np.max(q_table[newState])\n",
    "    updated_value = old_value + LEARNING_RATE * (reward + DISCOUNT_RATE * best_future - old_value)\n",
    "    return updated_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can test the Q Function to see if it works !\\\n",
    "If qTable[0, 1]'s value below is `-0.05`, congrats: you have successfully implemented the formula !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table after action:\n",
      "[[ 0.   -0.05  0.    0.  ]\n",
      " [ 0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "q_table = init_q_table(5,4)\n",
    "\n",
    "q_table[0, 1] = q_function(q_table, state=0, action=1, reward=-1, newState=3)\n",
    "\n",
    "print(\"Q-Table after action:\\n\" + str(q_table))\n",
    "\n",
    "assert(q_table[0, 1] == -LEARNING_RATE), f\"The Q function is incorrect: the value of qTable[0, 1] should be -{LEARNING_RATE}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up the GYM Environment\n",
    "\n",
    "#### What is Gym ?\n",
    "> Gym is a standard API for reinforcement learning, and a diverse collection of reference environments\n",
    ">\n",
    "> -- <cite>Gym Docs</cite>\n",
    "\n",
    "Let's get to the fun part:\n",
    "Read the documentation for FrozenLake [here](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "and find out how to load the environment.\n",
    "\n",
    "For now, we will set the `is_slippery` variable to `False` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write some code to load and make the FrozenLake environment:\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our environment is loaded, we need a function that performs a random action.\n",
    "\n",
    "An action is a number between 0 and the total number of possible actions.\n",
    "\n",
    "Try to find the value (inside `env`) that gives this total number of actions in the environment and store it in `total_actions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_actions = env.action_space.n\n",
    "\n",
    "assert(total_actions == 4), f\"There are a total of four possible actions in this environment. Your answer is {total_actions}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use `total_actions` to make a function which returns a random action each time it is called !\n",
    "\n",
    "There is also another way to do it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action(env):\n",
    "    return env.action_space.sample()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `random_action` function, you can run the code below which will display the first frame of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RcParams' object has no attribute '_get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m observation, reward, done, _, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Displaying the first frame of the game\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Printing game info\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mstates: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/tek/poc/q-learning-2025-nourrysebastieN/.venv/lib/python3.9/site-packages/matplotlib/_api/deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[1;32m    449\u001b[0m     warn_deprecated(\n\u001b[1;32m    450\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    453\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tek/poc/q-learning-2025-nourrysebastieN/.venv/lib/python3.9/site-packages/matplotlib/pyplot.py:2623\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2617\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[1;32m   2618\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mimshow\u001b[39m(\n\u001b[1;32m   2619\u001b[0m         X, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2620\u001b[0m         alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, extent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   2621\u001b[0m         interpolation_stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, filternorm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, filterrad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4.0\u001b[39m,\n\u001b[1;32m   2622\u001b[0m         resample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2623\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(\n\u001b[1;32m   2624\u001b[0m         X, cmap\u001b[38;5;241m=\u001b[39mcmap, norm\u001b[38;5;241m=\u001b[39mnorm, aspect\u001b[38;5;241m=\u001b[39maspect,\n\u001b[1;32m   2625\u001b[0m         interpolation\u001b[38;5;241m=\u001b[39minterpolation, alpha\u001b[38;5;241m=\u001b[39malpha, vmin\u001b[38;5;241m=\u001b[39mvmin,\n\u001b[1;32m   2626\u001b[0m         vmax\u001b[38;5;241m=\u001b[39mvmax, origin\u001b[38;5;241m=\u001b[39morigin, extent\u001b[38;5;241m=\u001b[39mextent,\n\u001b[1;32m   2627\u001b[0m         interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[1;32m   2628\u001b[0m         filternorm\u001b[38;5;241m=\u001b[39mfilternorm, filterrad\u001b[38;5;241m=\u001b[39mfilterrad, resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[1;32m   2629\u001b[0m         url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m   2630\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2631\u001b[0m     sci(__ret)\n\u001b[1;32m   2632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m~/tek/poc/q-learning-2025-nourrysebastieN/.venv/lib/python3.9/site-packages/matplotlib/pyplot.py:2237\u001b[0m, in \u001b[0;36mgca\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2235\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Figure\u001b[38;5;241m.\u001b[39mgca)\n\u001b[1;32m   2236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgca\u001b[39m():\n\u001b[0;32m-> 2237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgcf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgca()\n",
      "File \u001b[0;32m~/tek/poc/q-learning-2025-nourrysebastieN/.venv/lib/python3.9/site-packages/matplotlib/pyplot.py:842\u001b[0m, in \u001b[0;36mgcf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m manager\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mfigure\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfigure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tek/poc/q-learning-2025-nourrysebastieN/.venv/lib/python3.9/site-packages/matplotlib/_api/deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[1;32m    449\u001b[0m     warn_deprecated(\n\u001b[1;32m    450\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    453\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tek/poc/q-learning-2025-nourrysebastieN/.venv/lib/python3.9/site-packages/matplotlib/pyplot.py:783\u001b[0m, in \u001b[0;36mfigure\u001b[0;34m(num, figsize, dpi, facecolor, edgecolor, frameon, FigureClass, clear, **kwargs)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(allnums) \u001b[38;5;241m==\u001b[39m max_open_warning \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    774\u001b[0m     _api\u001b[38;5;241m.\u001b[39mwarn_external(\n\u001b[1;32m    775\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMore than \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_open_warning\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m figures have been opened. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    776\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFigures created through the pyplot interface \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using `matplotlib.pyplot.close()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    781\u001b[0m         \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m)\n\u001b[0;32m--> 783\u001b[0m manager \u001b[38;5;241m=\u001b[39m \u001b[43mnew_figure_manager\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfigsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframeon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframeon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFigureClass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFigureClass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    787\u001b[0m fig \u001b[38;5;241m=\u001b[39m manager\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mfigure\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fig_label:\n",
      "File \u001b[0;32m~/tek/poc/q-learning-2025-nourrysebastieN/.venv/lib/python3.9/site-packages/matplotlib/pyplot.py:358\u001b[0m, in \u001b[0;36mnew_figure_manager\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnew_figure_manager\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    357\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a new figure manager instance.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 358\u001b[0m     \u001b[43m_warn_if_gui_out_of_main_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_backend_mod()\u001b[38;5;241m.\u001b[39mnew_figure_manager(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/tek/poc/q-learning-2025-nourrysebastieN/.venv/lib/python3.9/site-packages/matplotlib/pyplot.py:336\u001b[0m, in \u001b[0;36m_warn_if_gui_out_of_main_thread\u001b[0;34m()\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_warn_if_gui_out_of_main_thread\u001b[39m():\n\u001b[1;32m    335\u001b[0m     warn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _get_required_interactive_framework(\u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    337\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(threading, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_native_id\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    338\u001b[0m             \u001b[38;5;66;03m# This compares native thread ids because even if Python-level\u001b[39;00m\n\u001b[1;32m    339\u001b[0m             \u001b[38;5;66;03m# Thread objects match, the underlying OS thread (which is what\u001b[39;00m\n\u001b[1;32m    340\u001b[0m             \u001b[38;5;66;03m# really matters) may be different on Python implementations with\u001b[39;00m\n\u001b[1;32m    341\u001b[0m             \u001b[38;5;66;03m# green threads.\u001b[39;00m\n\u001b[1;32m    342\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m threading\u001b[38;5;241m.\u001b[39mget_native_id() \u001b[38;5;241m!=\u001b[39m threading\u001b[38;5;241m.\u001b[39mmain_thread()\u001b[38;5;241m.\u001b[39mnative_id:\n",
      "File \u001b[0;32m~/tek/poc/q-learning-2025-nourrysebastieN/.venv/lib/python3.9/site-packages/matplotlib/pyplot.py:207\u001b[0m, in \u001b[0;36m_get_backend_mod\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03mEnsure that a backend is selected and return it.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03mThis is currently private, but may be made public in the future.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _backend_mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# Use __getitem__ here to avoid going through the fallback logic (which\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# will (re)import pyplot and then call switch_backend if we need to\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# resolve the auto sentinel)\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m     \u001b[43mswitch_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrcParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _backend_mod\n",
      "File \u001b[0;32m~/tek/poc/q-learning-2025-nourrysebastieN/.venv/lib/python3.9/site-packages/matplotlib/pyplot.py:265\u001b[0m, in \u001b[0;36mswitch_backend\u001b[0;34m(newbackend)\u001b[0m\n\u001b[1;32m    262\u001b[0m         rcParamsOrig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m backend_mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_module_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewbackend\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m required_framework \u001b[38;5;241m=\u001b[39m _get_required_interactive_framework(backend_mod)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:972\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/tek/poc/q-learning-2025-nourrysebastieN/.venv/lib/python3.9/site-packages/matplotlib_inline/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend_inline, config  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.2.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# we can't ''.join(...) otherwise finding the version number at build time requires\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# import which introduces IPython and matplotlib at build time, and thus circular\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# dependencies.\u001b[39;00m\n",
      "File \u001b[0;32m~/tek/poc/q-learning-2025-nourrysebastieN/.venv/lib/python3.9/site-packages/matplotlib_inline/backend_inline.py:236\u001b[0m\n\u001b[1;32m    231\u001b[0m                 ip\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39munregister(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_run_cell\u001b[39m\u001b[38;5;124m\"\u001b[39m, configure_once)\n\u001b[1;32m    233\u001b[0m             ip\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_run_cell\u001b[39m\u001b[38;5;124m\"\u001b[39m, configure_once)\n\u001b[0;32m--> 236\u001b[0m \u001b[43m_enable_matplotlib_integration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fetch_figure_metadata\u001b[39m(fig):\n\u001b[1;32m    240\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get some metadata to help with displaying a figure.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/tek/poc/q-learning-2025-nourrysebastieN/.venv/lib/python3.9/site-packages/matplotlib_inline/backend_inline.py:218\u001b[0m, in \u001b[0;36m_enable_matplotlib_integration\u001b[0;34m()\u001b[0m\n\u001b[1;32m    216\u001b[0m     backend \u001b[38;5;241m=\u001b[39m matplotlib\u001b[38;5;241m.\u001b[39mget_backend(auto_select\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 218\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[43mmatplotlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrcParams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ip \u001b[38;5;129;01mand\u001b[39;00m backend \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule://matplotlib_inline.backend_inline\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpylabtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activate_matplotlib\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RcParams' object has no attribute '_get'"
     ]
    }
   ],
   "source": [
    "observation, info = env.reset()\n",
    "\n",
    "# Performing an action\n",
    "action = random_action(env)\n",
    "observation, reward, done, _, info = env.step(action)\n",
    "\n",
    "# Displaying the first frame of the game\n",
    "plt.imshow(env.render())\n",
    "\n",
    "# Printing game info\n",
    "print(f\"actions: {env.action_space.n}\\nstates: {env.observation_space.n}\")\n",
    "print(f\"Current state: {observation}\")\n",
    "\n",
    "# Closing the environment\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment, there are **4 possible actions** for each of the **16 possible states**.\\\n",
    "Feel free to play around with the code above to get a better understanding of the API.\n",
    "\n",
    "## 3. Solving the environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got a Q Table, Q Function and our environment, we can create our `game_loop`.\n",
    "\n",
    "Call `q_function()` using the correct arguments it requires in the code below to update the `qTable[,]` using the values available in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_loop(env: gym.Env, q_table: np.ndarray, state: int, action: int) -> tuple:\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to see how the environment runs using your `game_loop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"human\")\n",
    "q_table = init_q_table(env.observation_space.n, env.action_space.n)\n",
    "\n",
    "state, info = env.reset()\n",
    "while (True):\n",
    "    env.render()\n",
    "    action = random_action(env)\n",
    "    q_table, state, done, reward = game_loop(env, q_table, state, action)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to see if our AI learns, we will launch the environment 1000 times and see how the Q-Table evolves.\n",
    "\n",
    "Notice how in the code below, we do not call `env.render()` each frame because if we did, our 1000 iterations would take a **lot** of time to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 20000\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "q_table = init_q_table(env.observation_space.n, env.action_space.n)\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    state, info = env.reset()\n",
    "    while (True):\n",
    "        # This time, we won't render the game each frame because it would take too long\n",
    "        action = random_action(env)\n",
    "        q_table, state, done, reward = game_loop(env, q_table, state, action)\n",
    "        if done:\n",
    "            break\n",
    "env.close()\n",
    "\n",
    "# Printing the QTable result:\n",
    "for states in q_table:\n",
    "    for actions in states:\n",
    "        if (actions == max(states)):\n",
    "            print(\"\\033[4m\", end=\"\")\n",
    "        else:\n",
    "            print(\"\\033[0m\", end=\"\")\n",
    "        if (actions > 0):\n",
    "            print(\"\\033[92m\", end=\"\")\n",
    "        else:\n",
    "            print(\"\\033[00m\", end=\"\")\n",
    "        print(round(actions, 3), end=\"\\t\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great ! We now have a nice Q-Table that indicates which action is best for each state."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it would probably be better for our agent to choose actions based on its experience, rather than at random.\n",
    "Write a function which chooses the best action for each given state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_action(q_table: np.ndarray, state: int) -> int:\n",
    "    \"\"\"\n",
    "    Write a function which finds the best action for the given state.\n",
    "\n",
    "    It should return its index.\n",
    "    \"\"\"\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to see the result of our training !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"human\")\n",
    "\n",
    "state, info = env.reset()\n",
    "while (True):\n",
    "    env.render()\n",
    "    action = best_action(q_table, state)\n",
    "    q_table, state, done, reward = game_loop(env, q_table, state, action)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If all went well, our AI should easily reach its goal !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the AI should probably try to explore all the different possibilities before it starts optimising its gain by following the Q Table...\\\n",
    "In order to solve this problem, we can use the Epsilon-Greedy strategy!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Epsilon Greedy ?\n",
    "\n",
    "Epsilon greedy is a strategy used to make sure that our AI explores its environment and doesn‚Äôt miss out on some cool easter eggs !\n",
    "\n",
    "![EpsilonGreedy](https://steadfast-ragdoll-d83.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa8444925-9940-46b2-b9e3-aae5ca792a74%2FUntitled.png?table=block&id=ce90b632-5d78-4cb0-8101-b326b91bf25d&spaceId=a008bb22-92df-498d-b32c-15988ddb70b9&width=770&userId=&cache=v2)\n",
    "\n",
    "<cite>[source](https://medium.com/analytics-vidhya/the-epsilon-greedy-algorithm-for-reinforcement-learning-5fe6f96dc870)</cite>\n",
    "\n",
    "```\n",
    "if random(0, 1) > epsilon-greedy:\n",
    "\treturn bestKnownAction\n",
    "else:\n",
    "\treturn randomAction\n",
    "```\n",
    "\t\t\n",
    "The higher the epsilon-greedy value is (from 0 to 1), the higher chance of the AI picking an action at random.\n",
    "\n",
    "The lower the epsilon-greedy value is (from 0 to 1 still), the higher chance of the AI picking an action it considers the most rewarding.\n",
    "\n",
    "In other words, with a high epsilon-greedy value, the robot pictured above might go to the right whereas it would probably choose the left path if it had a lower epsilon-greedy value, therefore missing out on a special discovery !\n",
    "\n",
    "\n",
    "\n",
    "Try to implement it into the following function that we'll use to determine which action the AI will choose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(epsilon: float, q_table: np.ndarray, state: int, env: gym.Env) -> int:\n",
    "    \"\"\"\n",
    "    Write a function to either return a random action or the best action using the functions\n",
    "    defined earlier.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function, the AI will explore all possible actions before it chooses the best state, reducing the risk of it missing some golden solutions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we're not done...\\\n",
    "You might have noticed that when we load our environment, we give it a certain argument:\\\n",
    "`is_slippery=False`\n",
    "\n",
    "This argument makes the game far easier !\n",
    "If you want a real challenge, set it to true."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the environment will be much harder to solve if the ice is slippery. Our agent's movements will be unpredictable and it will be impossible to make a 100% accurate AI.\n",
    "\n",
    "Therefore, keep track of the number of times the AI gets to its goal during our testing phase.\n",
    "\n",
    "##### Try to get the highest accuracy possible !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"rgb_array\")\n",
    "\n",
    "q_table = init_q_table(env.observation_space.n, env.action_space.n)\n",
    "\n",
    "# Training the AI\n",
    "epsilon = 1.0\n",
    "for i in range(10000):\n",
    "    epsilon = max(epsilon - 0.0001, 0)\n",
    "    state, info = env.reset()\n",
    "    while (True):\n",
    "        action = choose_action(epsilon, q_table, state, env)\n",
    "        q_table, state, done, reward = game_loop(env, q_table, state, action)\n",
    "        if done:\n",
    "            break\n",
    "# Testing the AI\n",
    "wins = 0.0\n",
    "for i in range(100):\n",
    "    state, info = env.reset()\n",
    "    while (True):\n",
    "        action = choose_action(0, q_table, state, env)\n",
    "        _, state, done, reward = game_loop(env, q_table, state, action)\n",
    "        if done:\n",
    "            if (reward > 0):\n",
    "                wins += 1\n",
    "            break\n",
    "\n",
    "print(f\"{round(wins / (i+1) * 100, 2)}% winrate\")\n",
    "print(wins)\n",
    "\n",
    "# Displaying the last frame of the game\n",
    "plt.imshow(env.render())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job ! You completed this workshop !\n",
    "If you want to continue in the wonderful world of reinforcement learning, you could try:\n",
    "- Try and increase the accuracy by using different methods of reinforcement learning.\n",
    "- A custom map for Frozen Lake. For example: `gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=True)`\n",
    "- A different environment from https://gymnasium.farama.org/\n",
    "- Going **deeper** with https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "- Trying what you've learned with games you know and love:\n",
    "    - https://gymnasium.farama.org/environments/atari/\n",
    "    - https://pypi.org/project/gym-super-mario-bros/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "97cba5719d159acad27bb7f37d182d27b1d38df28115fbd0d331121e7aca3605"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
